{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import higra as hg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from toolz import curry\n",
    "\n",
    "from cucim.skimage.transform import downscale_local_mean, rescale\n",
    "from cucim.skimage.filters import gaussian\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops\n",
    "from skimage import restoration\n",
    "\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tifffile import imread\n",
    "\n",
    "from dexp_dl.inference import ModelInference\n",
    "from dexp_dl.postprocessing import hierarchy\n",
    "from dexp_dl.models import hrnet, unet\n",
    "from dexp_dl.transforms import *\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{os.environ[\"HOME\"]}/Softwares/sparse-deconv-py')\n",
    "\n",
    "from sparse_recon.sparse_deconv import sparse_deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUT ###\n",
    "IM_PATH = 'bud stage - embryo3_1_crop_denoised.tif'\n",
    "WEIGHTS_PATH = 'logs/hrnet_bn/last.ckpt'\n",
    "CELL_CHANNEL = 0\n",
    "Z_SCALE = 2\n",
    "DATASET_PATH = IM_PATH.split('.')[0] + '.csv'\n",
    "\n",
    "th.cuda.set_device(0)\n",
    "\n",
    "### PARAMETERS ###\n",
    "PRED_THOLD = 0.25\n",
    "CUT_THOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de184c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def in_transform(image):\n",
    "    return th.Tensor(image).unsqueeze_(0).half()\n",
    "\n",
    "def out_transform(image):\n",
    "    return th.sigmoid(F.interpolate(image, scale_factor=2, mode='trilinear', align_corners=True))\n",
    "\n",
    "def normalize(image, upper_limit):\n",
    "    im_min = image.min()\n",
    "    image = (image - im_min) / (quantile - im_min)\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "net = hrnet.hrnet_w18_small_v2(pretrained=False, in_chans=1, num_classes=3, image_ndim=3)\n",
    "\n",
    "model = ModelInference(\n",
    "    net,\n",
    "    transforms=in_transform,\n",
    "    after_transforms=out_transform,\n",
    "    tile=(48, 96, 96), num_outputs=3,\n",
    ")\n",
    "\n",
    "model.load_weights(WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a18e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imread(IM_PATH)\n",
    "if image.shape[1] == 3:\n",
    "    image = image.transpose((1, 0, 2, 3))\n",
    "    \n",
    "# making it anisotropic\n",
    "image = downscale_local_mean(cp.asarray(image), (1, Z_SCALE, 1, 1)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb980c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = np.quantile(image[CELL_CHANNEL], 0.999)\n",
    "print('Quantile', quantile, 'Maximum', image[CELL_CHANNEL].max())\n",
    "\n",
    "# normalizing image\n",
    "norm_image = normalize(image[CELL_CHANNEL], quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "with th.cuda.amp.autocast():\n",
    "    pred = model(norm_image)\n",
    "    \n",
    "th.cuda.empty_cache()  \n",
    "\n",
    "# displaying\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image, channel_axis=0, name='Input image')\n",
    "viewer.add_image(pred[0], blending='additive', name='Cell prediction')\n",
    "viewer.add_image(pred[1], blending='additive', name='Distance map')\n",
    "if len(pred) > 2:\n",
    "    viewer.add_image(pred[2], blending='additive', name='Denoising')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing segmentation\n",
    "hiers = hierarchy.create_hierarchies(\n",
    "    pred[0] > PRED_THOLD,\n",
    "    pred[1],\n",
    "    hierarchy_fun=hg.watershed_hierarchy_by_area,\n",
    "    cache=True,\n",
    "    min_area=10,\n",
    "    min_frontier=0,\n",
    ")\n",
    "    \n",
    "for h in hiers:\n",
    "    h.cut_threshold = CUT_THOLD\n",
    "labels = hierarchy.to_labels(hiers, pred[0].shape)\n",
    "    \n",
    "labels_layer = viewer.add_labels(labels)\n",
    "labels_layer.contour = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfdbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blurring expressions\n",
    "if CELL_CHANNEL == 0:\n",
    "    expressions = image[1:].transpose((1, 2, 3, 0))\n",
    "elif CELL_CHANNEL == 2:\n",
    "    expressions = image[:CELL_CHANNEL].transpose((1, 2, 3, 0))\n",
    "else:\n",
    "    raise RuntimeError\n",
    "    \n",
    "viewer.add_image(expressions, name='original expressions', channel_axis=3)\n",
    "\n",
    "### DECONVOLUTION ###\n",
    "\n",
    "# change this to use/not use deconvolution\n",
    "use_deconv = True\n",
    "\n",
    "if use_deconv:\n",
    "    \"\"\"\n",
    "    Parameter description:\n",
    "    \n",
    "    background:\n",
    "        0 = no background noise\n",
    "        1 = low background noise\n",
    "        2 = high background noise\n",
    "    \n",
    "    fidelity:\n",
    "        higher values forces result to be closer to input\n",
    "    \n",
    "    sparsity:\n",
    "        higher values forces results to have more zeros --- it removes more blur/noise\n",
    "    \n",
    "    NOTE:\n",
    "        very HIGH `sparsity` and/or LOW `fidelity` might lead to an empty image as result\n",
    "    \"\"\"\n",
    "    deconv = np.zeros_like(expressions, dtype=float)\n",
    "    deconv[..., 0] = sparse_deconv(expressions[..., 0], [], background=1, fidelity=150, sparsity=10).get()\n",
    "    deconv[..., 1] = sparse_deconv(expressions[..., 1], [], background=1, fidelity=150, sparsity=10).get()\n",
    "    \n",
    "    viewer.add_image(deconv, name='deconvolved expressions', channel_axis=3)\n",
    "    \n",
    "    expressions = deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81298f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting expressions from segments\n",
    "props = regionprops(labels, expressions)\n",
    "exp_statistics = curry(np.mean, axis=0)\n",
    "\n",
    "# spreading expression to segments\n",
    "mask = np.zeros(labels.shape + (2,), dtype=expressions.dtype)\n",
    "for p in props:\n",
    "    exp1, exp2 = exp_statistics(p.intensity_image[p.image])\n",
    "    mask[p.slice + (0,)][p.image] = exp1\n",
    "    mask[p.slice + (1,)][p.image] = exp2\n",
    "\n",
    "exp1_layer, exp2_layer = viewer.add_image(mask, name='orig. label exp.', channel_axis=3)\n",
    "exp1_layer.colormap = 'green'\n",
    "exp2_layer.colormap = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79669b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "for p in props:\n",
    "    exp1, exp2 = exp_statistics(p.intensity_image[p.image])\n",
    "    if exp1 > exp1_layer.contrast_limits[0] and exp2 > exp2_layer.contrast_limits[0]:\n",
    "        df.append([p.label, exp1, exp2, *p.centroid])\n",
    "\n",
    "df = pd.DataFrame(df, columns=['label', 'exp1', 'exp2', 'z', 'y', 'x'])\n",
    "\n",
    "# computing colors\n",
    "green = np.array([[0, 1, 0, 1]])  # exp1\n",
    "red = np.array([[1, 0, 0, 1]])  # exp2\n",
    "\n",
    "def normalize_column(df, column, upper_quantile=1):\n",
    "    col = df[column].values.copy()\n",
    "    minimum = col.min()\n",
    "    col -= minimum\n",
    "    quantile = np.quantile(col, upper_quantile)\n",
    "    col /= quantile\n",
    "    \n",
    "    def norm_fun(x):\n",
    "        return min(1, max(0, (x - minimum) / quantile))\n",
    "    \n",
    "    return np.clip(col, 0, 1), norm_fun\n",
    "\n",
    "blending_exp1, normfun_exp1 = normalize_column(df, 'exp1')\n",
    "blending_exp2, normfun_exp2 = normalize_column(df, 'exp2')\n",
    "\n",
    "colors = green * blending_exp1[:, None] + red * blending_exp2[:, None]\n",
    "colors[:,3] = 1 # alpha channel\n",
    "\n",
    "plt.scatter(x=df['exp1'], y=df['exp2'], c=colors)\n",
    "plt.xlabel('exp. 1'); plt.ylabel('exp. 2')\n",
    "\n",
    "df.to_csv(DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357aa9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching segments color to plot\n",
    "mask = np.zeros(labels.shape + (2,), dtype=np.float16)\n",
    "for p in props:\n",
    "    exp1, exp2 = exp_statistics(p.intensity_image[p.image])\n",
    "    if exp1 > exp1_layer.contrast_limits[0] and exp2 > exp2_layer.contrast_limits[0]:\n",
    "        mask[p.slice + (0,)][p.image] = normfun_exp1(exp1)\n",
    "        mask[p.slice + (1,)][p.image] = normfun_exp2(exp2)\n",
    "\n",
    "layers = viewer.add_image(mask, name='selected label exp.', channel_axis=3)\n",
    "layers[0].colormap = 'green'\n",
    "layers[1].colormap = 'red'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
